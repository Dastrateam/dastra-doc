# Analyse de risque et valeur ajoutée

Dastra vous propose de procéder à une **analyse de risque** de vos systèmes d'IA. Dans cette partie du formulaire, vous pouvez assigner un des 4 niveaux de risque à chaque système. **Ces niveaux de risque ont été définis par l'IA Act** et sont classés du risque minimal au risque inacceptable. Nous vous recommandons d'ajouter une description afin de justifier le risque du système.

#### 🔹 Niveaux de risque selon l’AI Act

L’AI Act classe les systèmes d’IA en quatre catégories de risque, chacune impliquant des obligations spécifiques :

***

**1. Risque minimal (Minimal risk)**

✔️ **Définition :**\
Systèmes d’IA présentant un risque négligeable ou nul pour les droits fondamentaux ou la sécurité des personnes.

💡 **Exemples :**

* Filtres anti-spam
* Moteurs de recommandation de contenus (sans impact significatif)
* IA utilisées dans des jeux vidéo pour l’expérience utilisateur

🔎 **Obligations :**\
Aucune obligation particulière au titre de l’AI Act, mais les règles générales (ex. conformité RGPD) restent applicables.

***

**2. Risque limité (Limited risk)**

✔️ **Définition :**\
Systèmes d’IA nécessitant des obligations de **transparence** vis-à-vis des utilisateurs, sans impact significatif sur leurs droits.

💡 **Exemples :**

* Chatbots conversationnels (information obligatoire de l’utilisateur sur l’interaction avec une IA)
* Deepfake générés à des fins de divertissement (avec mention claire)

🔎 **Obligations :**

* Informer l’utilisateur qu’il interagit avec un système d’IA
* Garantir la transparence sur la nature générée ou modifiée du contenu

***

**3. Risque élevé (High risk)**

⚠️ **Définition :**\
Systèmes d’IA présentant un risque significatif pour la santé, la sécurité ou les droits fondamentaux des personnes. Ils sont soumis à un **cadre strict** de conformité et d’évaluation.

💡 **Exemples :**

* Systèmes de scoring de crédit bancaire
* Recrutement automatisé (tri de candidatures)
* Systèmes biométriques à finalité sécuritaire (sauf interdits)
* IA utilisées dans les dispositifs médicaux ou infrastructures critiques

🔎 **Obligations :**

* Évaluation de conformité avant mise sur le marché
* Gestion des risques
* Gouvernance des données utilisées
* Transparence et documentation technique détaillée
* Surveillance humaine et auditabilité

***

**4. Risque inacceptable (Prohibited / Unacceptable risk)**

⛔ **Définition :**\
Systèmes d’IA interdits car contraires aux valeurs de l’UE ou portant atteinte aux droits fondamentaux.

💡 **Exemples :**

* Systèmes de scoring social à la chinoise
* Reconnaissance biométrique à distance en temps réel à des fins répressives (hors exceptions strictes)
* Manipulation cognitive subliminale des personnes

🔎 **Obligations :**

* **Interdiction totale** de mise sur le marché ou d’utilisation dans l’UE.

***

✅ **Astuce :**\
Attribuer le bon niveau de risque à vos systèmes d’IA vous permettra de :

* Déterminer rapidement les **obligations applicables**
* Prioriser les actions de mise en conformité
* Communiquer de manière claire auprès de vos équipes métiers et juridiques

***



<figure><img src="../../.gitbook/assets/Capture d&#x27;écran 2024-06-14 142759.png" alt=""><figcaption><p>Analyse de risque d'un système d'IA</p></figcaption></figure>

#### **Valeur ajoutée d’un système d’IA**

La section suivante vous invite à évaluer la **valeur ajoutée** de chaque système pour votre organisation.\
Vous devez sélectionner **l’un des trois niveaux de valeur**, allant de **faible** à **élevé**, et expliquer en quoi ce système apporte un bénéfice concret, tel que :

* Un **gain de performance opérationnelle**
* Une **amélioration de la qualité des services**
* Une **réduction des risques ou des coûts**



<figure><img src="../../.gitbook/assets/Capture d&#x27;écran 2024-06-14 143019.png" alt=""><figcaption><p>Valeur ajoutée d'un système d'IA</p></figcaption></figure>

#### **Pourquoi remplir ces deux parties ?**

Renseigner à la fois le **niveau de risque** et la **valeur ajoutée** vous permettra de déterminer rapidement si un système d’IA constitue plutôt :

✅ **Une opportunité stratégique** pour votre organisation, ou\
⚠️ **Un risque disproportionné** au regard de sa faible utilité.

Par exemple, un système présentant un **risque élevé** mais une **valeur ajoutée faible** pourrait ne pas justifier son maintien en production.

***

✅ **Astuce :** Cette analyse combinée facilite vos décisions d’arbitrage et l’alignement de votre portefeuille IA sur vos priorités stratégiques et réglementaires.
